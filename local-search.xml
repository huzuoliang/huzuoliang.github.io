<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</title>
    <link href="/2022/11/24/Multi-Task-Learning-Using-Uncertainty-to-Weigh-Losses-for-Scene-Geometry-and-Semantics/"/>
    <url>/2022/11/24/Multi-Task-Learning-Using-Uncertainty-to-Weigh-Losses-for-Scene-Geometry-and-Semantics/</url>
    
    <content type="html"><![CDATA[<h2 id="论文解读">论文解读</h2><p>Multi-task learning concerns the problem of optimising a model withrespect to multiple objectives. The naive approach to combining multiobjective losses would be to simply perform a weighted linear sum of thelosses for each individual task:<br> <imgsrc="/img/uncertainty_weigh_losses/naive_loss.png" /><br> However, thereare a number of issues with this method. Namely, model performance isextremely sensitive to weight selection, wi, as illustrated in Figure 2.These weight hyper-parameters are expensive to tune, often taking manydays for each trial. Therefore, it is desirable to find a moreconvenient approach which is able to learn the optimal weights</p><h3 id="mathematical-formulation">Mathematical Formulation</h3><p>First the paper defines multi-task likelihoods:<br> - For regressiontasks, likelihood is defined as a Gaussian with mean given by the modeloutput with an observation noise scalar σ:<br> <imgsrc="/img/uncertainty_weigh_losses/reg_likelihood.png" /><br> - Forclassification, likelihood is defined as:<br> <imgsrc="/img/uncertainty_weigh_losses/class_likelihood_1.png" /><br>where:<br> <imgsrc="/img/uncertainty_weigh_losses/class_likelihood_0.png" /><br></p><p>In maximum likelihood inference, we maximise the log likelihood ofthe model. In regression for example:<br> <imgsrc="/img/uncertainty_weigh_losses/reg_loglikelihood.png" /><br> σ isthe model’s observation noise parameter - capturing how much noise wehave in the outputs. We then maximise the log likelihood with respect tothe model parameters W and observation noise parameter σ.<br></p><p>Assuming two tasks that follow a Gaussian distributions:<br> <imgsrc="/img/uncertainty_weigh_losses/two_task.png" /><br> The loss willbe:<br> <img src="/img/uncertainty_weigh_losses/total_loss_h.png" /><br><img src="/img/uncertainty_weigh_losses/loss7.png" /><br> This meansthat W and σ are the learned parameters of the network. W are the wightsof the network while σ are used to calculate the wights of each taskloss and also to regularize this task loss wight.</p><p>However, the extension to classification likelihoods is moreinteresting. We adapt the classification likelihood to squash a scaledversion of the model output through a softmax function:<br> <imgsrc="/img/uncertainty_weigh_losses/sf.png" /><br> with a positive scalarσ. This can be interpreted as a Boltzmann distribution (also calledGibbs distribution) where the input is scaled by σ2 (often referred toas temperature). This scalar is either fixed or can be learnt, where theparameter’s magnitude determines how ‘uniform’ (flat) the discretedistribution is. This relates to its uncertainty, as measured inentropy. The log likelihood for this output can then be written as<br><img src="/img/uncertainty_weigh_losses/sf1.png" /><br> assume that amodel’s multiple outputs are composed of a continuous output y1 and adiscrete output y2, modelled with a Gaussian likelihood and a softmaxlikelihood, respectively. Like before, the joint loss, L(W, σ1, σ2), isgiven as:<br> <img src="/img/uncertainty_weigh_losses/rgsf.png" /><br><u><strong><font color=#FF000>In practice, we train the network topredict the log variance, s := log σ2. This is because it is morenumerically stable than regressing the variance, σ2, as the loss avoidsany division by zero. The exponential mapping also allows us to regressunconstrained scalar values, where exp(−s) is resolved to the positivedomain giving valid values for variance.</font></strong></u></p><h2 id="代码实现">代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>(<span class="hljs-params">model_config</span>):<br>  inputs = []<br>  outputs = []<br><br>  <span class="hljs-keyword">return</span> LossesUWLModel(inputs=inputs, outputs=outputs)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LossesUWLModel</span>(tf.keras.Model):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        self.sigma = &#123;&#125;<br>        <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> self.output_names:<br>            self.sigma[o] = tf.Variable(tf.random.uniform(shape=[], minval=<span class="hljs-number">0.2</span>, maxval=<span class="hljs-number">1</span>, seed=<span class="hljs-number">10</span>), dtype=tf.float32,<br>                                        trainable=<span class="hljs-literal">True</span>,<br>                                        constraint=tf.keras.constraints.NonNeg(),<br>                                        name=o + <span class="hljs-string">&#x27;sigma&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_step</span>(<span class="hljs-params">self, data</span>):<br>        <span class="hljs-comment"># Unpack the data. Its structure depends on your model and</span><br>        <span class="hljs-comment"># on what you pass to `fit()`.</span><br>        x, y = data<br><br>        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:<br>            y_pred = self(x, training=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># Forward pass</span><br>            <span class="hljs-comment"># Compute the loss value</span><br>            <span class="hljs-comment"># (the loss function is configured in `compile()`)</span><br><br>            task_loss = []<br>            total_loss = <span class="hljs-number">0.0</span><br>            <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> self.output_names:<br>                i = self.output_names.index(o)<br>                loss_i = self.loss[o](y_true=y[o], y_pred=y_pred[i])<br>                task_loss.append(loss_i)<br>                total_loss = tf.add(total_loss, tf.divide(loss_i, self.sigma[o] ** <span class="hljs-number">2</span>))<br>                total_loss = tf.add(total_loss, tf.math.log(self.sigma[o] ** <span class="hljs-number">2</span>))<br><br>        trainable_vars = self.trainable_variables<br>        gradients = tape.gradient(total_loss, trainable_vars)<br>        <span class="hljs-comment"># Update weights</span><br>        self.optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, trainable_vars))<br>        <span class="hljs-comment"># Update metrics (includes the metric that tracks the loss)</span><br>        self.compiled_metrics.update_state(y, y_pred)<br><br>        <span class="hljs-keyword">return</span> &#123;m.name: m.result() <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.metrics&#125;<br><br><br>model = build_model(FLAGS.model_config)<br>model.<span class="hljs-built_in">compile</span>(<br>    optimizer=tf.keras.optimizers.RMSprop(FLAGS.lr),<br>    loss=&#123;<br>        <span class="hljs-string">&quot;a&quot;</span>: tf.keras.losses.BinaryCrossentropy(name=<span class="hljs-string">&#x27;loss&#x27;</span>, label_smoothing=<span class="hljs-number">0.1</span>),<br>        <span class="hljs-string">&quot;b&quot;</span>: tf.keras.losses.BinaryCrossentropy(name=<span class="hljs-string">&#x27;loss&#x27;</span>),<br>    &#125;,<br>    metrics=&#123;<br>        <span class="hljs-string">&quot;a&quot;</span>: [tf.keras.metrics.BinaryCrossentropy(), tf.keras.metrics.AUC(name=<span class="hljs-string">&#x27;auc&#x27;</span>)],<br>        <span class="hljs-string">&quot;b&quot;</span>: [tf.keras.metrics.BinaryCrossentropy(), tf.keras.metrics.AUC(name=<span class="hljs-string">&#x27;auc&#x27;</span>)],<br>    &#125;,<br>    <span class="hljs-comment"># run_eagerly=True</span><br>)<br></code></pre></td></tr></table></figure><h2 id="论文">论文</h2><p><img src="/img/uncertainty_weigh_losses/c-01.png" /> <imgsrc="/img/uncertainty_weigh_losses/c-03.png" /> <imgsrc="/img/uncertainty_weigh_losses/c-04.png" /> <imgsrc="/img/uncertainty_weigh_losses/c-05.png" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多目标排序</tag>
      
      <tag>Loss权重优化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/09/13/hello-world/"/>
    <url>/2022/09/13/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><p>$ f(x) = a+b $</p><p><span class="math display">\[ f(x) = a+b \]</span></p><p><span class="math display">\[\sqrt{x} + \sqrt{x^{2}+\sqrt{y}} =\sqrt[3]{k_{i}} - \frac{x}{m}\]</span></p><p><span class="math display">\[  \lim_{x \to \infty} x^2_{22} -\int_{1}^{5}x\mathrm{d}x + \sum_{n=1}^{20} n^{2} = \prod_{j=1}^{3}y_{j}  + \lim_{x \to -2} \frac{x-2}{x} \]</span></p><p><span class="math display">\[\begin{bmatrix}1 &amp; 2 &amp; \cdots \\67 &amp; 95 &amp; \cdots \\\vdots  &amp; \vdots &amp; \ddots \\\end{bmatrix}\]</span></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>这是一句话<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="参考资料1">[1]</span></a></sup></p><h2 id="参考">参考</h2><p>常用工具: <a href="https://pdf2png.com/">pdf2png</a></p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>这是对应的脚注<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:1" class="footnote-text"><span>参考资料1<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>参考资料2<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Fluid</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
